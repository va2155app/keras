{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#STEP 1. 引入相關的函數庫\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 專案的根目錄路徑\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# 置放訓練資料的目錄\n",
    "DATA_PATH = os.path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "# 訓練資料檔\n",
    "DATA_FILE = os.path.join(DATA_PATH, \"cmn-tw.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2. 相關的參數\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # 訓練時的批次數量\n",
    "epochs = 100 # 訓練循環數\n",
    "latent_dim = 256 # 編碼後的潛在空間的維度(dimensions of latent space)\n",
    "num_samples = 10000 # 用來訓練的樣本數\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.資料的前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 73\n",
      "Number of unique output tokens: 2165\n",
      "Max sequence length for inputs: 33\n",
      "Max sequence length for outputs: 22\n"
     ]
    }
   ],
   "source": [
    "# 資料向量化\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set() # 英文字符集\n",
    "target_characters = set() # 中文字符集\n",
    "lines = open(DATA_FILE, mode=\"r\", encoding=\"utf-8\").read().split('\\n')\n",
    "\n",
    "# 逐行的讀取與處理\n",
    "for line in lines[: min(num_samples, len(lines)-1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    \n",
    "    # 我們使用“tab”作為“開始序列[SOS]”字符或目標，“\\n”作為“結束序列[EOS]”字符。 <-- **重要\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "            \n",
    "input_characters = sorted(list(input_characters)) # 全部輸入的字符集\n",
    "target_characters = sorted(list(target_characters)) # 全部目標字符集\n",
    "\n",
    "num_encoder_tokens = len(input_characters) # 所有輸入字符的數量\n",
    "num_decoder_tokens = len(target_characters) # 所有輸目標字符的數量\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts]) # 最長的輸入句子長度\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts]) # 最長的目標句子長度\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "# 輸入字符的索引字典\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "\n",
    "# 輸目標字符的索引字典\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# 包含英文句子的one-hot向量化的三維形狀數組（num_pairs，max_english_sentence_length，num_english_characters）\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# 包含中文句子的one-hot向量化的三維形狀數組（num_pairs，max_chinese_sentence_length，num_chinese_characters）\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# decoder_target_data與decoder_input_data相同，但是偏移了一個時間步長。 \n",
    "# decoder_target_data [:, t，：]將與decoder_input_data [：，t + 1，：]相同\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# 把資料轉換成要用來訓練用的張量資料結構 <-- 重要\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        \n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4.構建網絡架構\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None, 73)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None, 2165)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 256), (None, 337920      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  2480128     decoder_input[0][0]              \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 2165)   556405      decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,374,453\n",
      "Trainable params: 3,374,453\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ===== 編碼 (encoder) ====\n",
    "\n",
    "# 定義輸入的序列\n",
    "# 注意：因為輸入序列長度(timesteps)可變的情況，使用input_shape =（None，num_features）\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), name='encoder_input') \n",
    "encoder = LSTM(latent_dim, return_state=True, name='encoder_lstm') # 需要取得LSTM的內部state, 因此設定\"return_state=True\"\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# 我們拋棄掉`encoder_outputs`因為我們只需要LSTM cell的內部state參數\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# ==== 解碼 (decoder) ====\n",
    "\n",
    "# 設定解碼器(decoder)\n",
    "# 注意：因為輸出序列的長度(timesteps)是變動的，使用input_shape =（None，num_features）\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_input')\n",
    "\n",
    "# 我們設定我們的解碼器回傳整個輸出的序列同時也回傳內部的states參數\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "\n",
    "# 在訓練時我們不會使用這些回傳的states, 但是在預測時我們會用到這些states參數\n",
    "# **解碼器的初始狀態是使用編碼器的最後的狀態(states)**\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states) #我們使用`encoder_states`來做為初始值(initial state) <-- 重要\n",
    "\n",
    "# 接密集層(dense)來進行softmax運算每一個字符可能的機率\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_output')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 定義一個模型接收encoder_input_data` & `decoder_input_data`做為輸入而輸出`decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 打印出模型結構\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAccAAAFgCAYAAADO5bLkAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3df3Ac5X3H8c9ZsvkVYuOA8TiNUwLYaSlR2rRYNiQG10ANnDCNf0lCgAG7p8ykDR3TAj2FtGaGyXAeGOpWimQyASpLsR1sdIBpplaIiS2Z4M65TIbIA05PxG1OnU7ukrjU9Y+nfzi72XvuTrqT7m7vpPdr5sa+3Wd3v7fa3c/t7nN3AWOMEQAAcE3zuwAAACoN4QgAgIVwBADAQjgCAGCptQf87Gc/00MPPaQzZ874UQ8wpbS0tCgYDPpdBgBLxpljf3+/ent7/agFmFJ27tzJvgZUqIwzR8eOHTvKWQcw5TQ3N/tdAoAcuOcIAICFcAQAwEI4AgBgIRwBALAQjgAAWAhHAAAshCMAABbCEQAAC+EIAICFcAQAwEI4AgBgIRwBALAQjgAAWAhHAAAsUyYcR0ZG1Nvbq4aGhrIts62tTW1tbWVbHgCgOHL+nuNk8/jjj6ujo8PvMsoqlUpp1qxZMsbkPU0gEMg6vJB5FItdfyXVBmBymzJnju3t7WVf5ubNm7V58+ayL9exf//+gqcxxiiZTLrPk8mkb+Fj12+MUSKRcJ/7WRuAyW3KhONUk0ql1NXVNa5pZ86cmfX/5ZSr/jlz5rj/96s2AJNf0cJxZGREW7ZsUSAQUENDg/r7+93h3nt90WjUbTM8PJw2j1Qqpd7eXgUCAQUCgawHx2xtRkZGRm3X0NCgo0ePFlx3NBpVQ0ODUqmUWltbC7p/aL/ufNaDd5mS1NXVpUAgoNbW1rT6ndfuvcxoD4tEIopGo2njpPHfB62U+gvhBKwzfVtbW9rf23ls2bLFncY7zvu6SrGNAKhgxtLd3W2yDB5VIpEwwWDQ9PT0GGOM2bdvn5FkYrGYCQaDRpKRZAYGBowxxsTjcSPJhEKhtPkEg0ETDofd56FQKO2506azszNtucFg0CSTyYx2oVDIHd7T0+PWMZ66Y7FYRr2j8U5vP8+1Hpzx3jbJZNKEQiEjyQwNDbl126/FmZd3mP3cGGPC4XDGOs3GnrZS6h9tuM1ZbiKRyKh1YGAg6zbovNZEIuHWWqptpKmpyTQ1NeXdHkD5FCUcneBJm7HkHoSzHczsYc48nIOSMecOYMFg0H3uHJjsNpLcg5cxxvT19aUdjI05d5DOtcyx6raDN1/5HOzzaROLxYwkE4lEJjyv8dZeSfXn+7rC4XBaWNnTRSIRI8nE4/G0Wr3bUim3EcIRqFxFCUfvO2j7YUx+B0JnHqNxzgS8nNDzhmi2dqMts5C6C1GscCz2vMZTeyXVX+jrisfjbhB6p3NC27kSYcy5wPSGZSm3EcIRqFwBY9K7+23fvl3Nzc3j6v6fa5ps4+1hY81jtDb5zqvQZeZT02jyWV4xax/POs239kqqv5DX1dXVpWg0qkgkooULF2ZM19raqo6ODreH7iOPPJLWs7mU20hzc7Mkqbu7u+BpAZRWUXur5ur0ko9gMChJOnLkyJhtsnXACYVC4172ROoup4m8xkpQrvpbW1slSb29vdq4caO2bt2qBQsWjFrT3r17tX//ft17771Z21XLNgKgOIoSjp2dnZKkF198UalUStJvevjlywm+jo4Odx7Dw8PugU6SmpqaJEnHjh1zhzltV69enVHPaEFbrLrLwTkw33bbbT5XMj7lrH9wcFBLly6VJDU2NkqS5s+fn7N9XV2dQqGQGhsb1dXVpfr6+rTx1bKNACgy+zrreHurKss9mXg8njbO6bTg7Rxj9wr0Th8KhTI61Ti9U53penp6MnoIOj0Tg8Gge//I6czjzLeQusfDO30ikch7PTjPnU4hyWTShMPhtHuqxpiMHqBOxyTv63PWZyKRcDvD5NNb1VuXU2ul1D/a38WZRywWS5s+Ho+boaGhjFrt6bz3Hh2l3Ea45whUrqKEozHnAikcDrsHNyeU7INKrmHGnDsQOfMIh8Npweht09nZmXYQztZTMB6PuwfgUCiU1iXfe3DMp277wJ6PbAfUfNaD83/vRwU6OzszXmM8HnfH9/X1GWNMxutzOpyEw2F32FjhOFbdftafb23Osuzpnd6r3g43jmAwmHV7c2otxTZCOAKVqygdclA8E+0E5LdqrD+VSmV0xCkHOuQAlYuvj8OUt2PHjrR71gBAOFYQby/cbD1yK1011d/W1pb2NXHLli3zuyQAFWTK/GRVseT7HZ/juax4+eWXp/2/mi5NStVVv9ODtbOzUxs2bPC5GgCVhnAsUCkP+JUcJvmopvo3bNhAKALIicuqAABYCEcAACyEIwAAFsIRAAAL4QgAgIVwBADAQjgCAGAhHAEAsBCOAABYCEcAACyEIwAAFsIRAAAL4QgAgCXnr3KsWbOmnHUAU87OnTvV1NTkdxkAssg4c1y2bJnWrVvnRy2YgP3791f8Dwwj3erVq9nXgAoVMNX0I3zIKRAIqLu7mzMRACgC7jkCAGAhHAEAsBCOAABYCEcAACyEIwAAFsIRAAAL4QgAgIVwBADAQjgCAGAhHAEAsBCOAABYCEcAACyEIwAAFsIRAAAL4QgAgIVwBADAQjgCAGAhHAEAsBCOAABYCEcAACyEIwAAFsIRAAAL4QgAgIVwBADAQjgCAGAhHAEAsBCOAABYCEcAACyEIwAAFsIRAAAL4QgAgIVwBADAQjgCAGAJGGOM30WgMN/5znf06KOPat68ee6wAwcOaOHChbr00kslSclkUjfccIO2bt3qV5kAULUIxyrU1tamJ554Iq+2/HkBoHBcVq1CjY2NY7aZPn26vva1r5W+GACYhDhzrFK/93u/px/96Eejtvnxj3+shQsXlqkiAJg8OHOsUnfffbemT5+edVwgENBnPvMZghEAxolwrFKNjY06ffp01nE1NTW69957y1wRAEweXFatYvX19frhD3+os2fPpg0PBAL64IMP9PGPf9ynygCgunHmWMXuvfdeBQKBtGHTpk3TkiVLCEYAmADCsYqtWrUqY1ggENA999zjQzUAMHkQjlXssssu00033aSamhp3WCAQyBqaAID8EY5V7p577nE/6F9TU6Obb75Zs2fP9rkqAKhuhGOVW7lypfuRDmOM7r77bp8rAoDqRzhWuYsvvli33367JGnGjBm68847fa4IAKpfbalmfPr0afX19enMmTOlWgR+7VOf+pT772uvveZzNVNDfX29PvGJT5Rk3h988IEGBwdLMm8AmbLuz6ZEdu/ebSTx4DEpH+vXry/VrmPWr1/v++vjwWMqPbLtzyU7c/yf//kfSeJXITDpNDc36+TJkyWb/8mTJ9XU1KTu7u6SLQPAObn2Z+45AgBgIRwBALAQjgAAWAhHAAAshCMAABbCEQAAC+EIAICFcAQAwEI4AgBgIRwBALAQjgAAWAhHAAAshCMAABbCEQAAC+FYoJGREfX29qqhoWFSLg+TRyVuO37U1NbWpra2trItD5MD4Vigxx9/XI2NjYpGoxW7vFQqpUAgUMKqfiMQCGR9jGZwcFCtra0KBAJqbW1Vf39/Rs255pvvY3BwcNTlF1JvtSr3tpqPSqyp1MazP45nvyqVfPfNyYZwLFB7e3vFL2///v0lqCQ7Y4wSiYT7PJlMjvoD14ODg1q8eLGWLl0qY4za29v1sY99TC0tLRlte3p6ZIxxH95lOo+enh53WDwed9s8//zzOWvwjkskEpP2B7nLva3mw4+aNm/erM2bN5d9uY7x7I/GGCWTSff5WPtVKdn1F7rPVyvCcZJJpVLq6uoq6zLnzJnj/n/mzJmjtnWCad26de6wurq6rAcvb5tcVqxY4f5//vz5kqRIJKKOjg4NDw9ntB8eHtZVV12VtXag2CayP3r3pbH2q1LJVX8h+3y1qrhwHBkZ0ZYtWxQIBNTQ0KD+/n53uPdeRTQaddvYB8FUKqXe3l73dD/bHzdbm5GRkVHbNTQ06OjRowXXHY1G1dDQoFQqpdbW1qLc/3CW5dTtXNaIRCLuJSvnteVad62tre66c16jd5hU/Ps1x48flyQdOXIkbXhdXV3ac+9Z4GhmzpyZ0Xb58uWSpIMHD2a0P3jwoDt+spnotpptPtW+/9jbfj7HEe8yJamrq8vdN7z1Z7ukaA/Ltj9K49+vKqX+QjgB60zf1taW9vd2Hlu2bHGn8Y7zvq5yHmNlSqS7u9sUOvtEImGCwaDp6ekxxhizb98+I8nEYjETDAaNJCPJDAwMGGOMicfjRpIJhUJp8wkGgyYcDrvPQ6FQ2nOnTWdnZ9pyg8GgSSaTGe1CoZA7vKenx61jPHXHYrGMesdiLy8SiZh4PG6MMSaZTJpwOJw23m7vrSEWixljjBkYGHDX3WjrMxwOZ6y7fGrMJRaLuW07Ozsz1vdEl+GMD4VCWds6ry3ferNpamoyTU1N45q2lPOf6Lbqnc9k2X+809vPc233znhvm2Qy6W5TQ0NDbt32a3HmNdr+aMz496tKqX+04TZnuYlEIqNW73HIFgwGTSKRcGst1TaSa3+rqHB0dhwvSe5GlO2PYQ9z5uGsVGPO/QGCwaD73FmxdhtJ7so3xpi+vr60jcmYcxtZrmWOVXchQTDaa7RrdzbyXO0nOmw8NY5maGjI3WGcdZ7PuikkHJ2/sXNwMOZcMO/bt6/gem2VGI7F2lanyv4z1nafrY3zxi4SiUx4XuOtvZLqz/d1hcPhtLCyp4tEIkaS+4bfqdW7LZVyG6mKcPS+A7AfxuT3h3TmMZpsZxXOTus9COQ6+8i1zELqLoQ9vVNXrlCp9HB0DAwMpIVkX1/fhJdh79T2WfBE6nVUYjgWa1udCvtPMQOhmsKx2PUX+rri8bgbhN7pnNB2rkQYk351zJjSbiNVEY5jvcBibYgT2XDGs8xi79xDQ0NpG4v3nWCu5VViODqcM5OxArLQcHTebcbjcZNIJNLeiU62cCzntlrt+0+lhMt4aq+k+gt5XZ2dnSYYDJqhoaGs0zlvpJLJpHsJuJBllWJ/rrgOOZJy3rTPRzAYlJTZ4SNbm2wdCEKh0LiXPZG6C7FgwQL19fUpFospFApp06ZNaTezK1Fra6ukczf1U6lU2rj6+npt3bpVkor64fAlS5ZIOtcJp7+/332O3NvqVNh/Jmoir7ESlKt+Z5/v7e3Vxo0btXXrVi1YsGDUmvbu3av9+/fr3nvvzdqunNtIRYVjZ2enJOnFF190D6BOD6V8OTtuR0eHO4/h4WH3DyVJTU1NkqRjx465w5y2q1evzqhntANFseouhBMwdXV1am9vVywW06ZNm0qyrGIYHBzU0qVL3eeHDx/OaON8DMP5+xXD/PnzFQ6H1djYqOPHj7vLmIyKta1Ohf1nvJwD82233eZzJeNTzvq9+3xjY6Mkjbr/1dXVKRQKqbGxUV1dXaqvr08b78s2Mq7z0DyMt7eqslxTdi6LOc+d+2zem/t2rybv9KFQKKNTgNO7zpmup6cn41Te6VkVDAbd699OZwRnvoXUPR7e6Z1apXM3op2anGv5Duf1JxIJE4lEsq67bPPNNiyfXnWjvUano4bTG9Jpt2/fvrS/o3MJ1Ntrcqz1kKuNd7xzP8M733zmNZpKvKxajG3VGT+Z9598jyPOc+dSvNMr3HtP1RiT0QPU2d69r8/eH43Jb7/y1pVtv/Wz/kL2eWf6eDyedlnV3vec6bz3Hh2l3Eaq4p6jMed2KOejCaFQyN2p7JWSa5gx51akM49wOJy2Y3vbdHZ2pm1E2Tq3xONxdwMKhUJpXYq9f9x86rY3zHzket3Ohipl3nN0AiEcDmfdqApZn2PtxNk22GwPZ9068x0aGkpb/7n+TqMtY6w2jmw95Uab11gqMRyNmfi26pjM+0+h+4L3owLZPnoUj8cz7pfbr8/eH40p3n7lR/2F7vP29E7vVXu7c5ad6zhQqm0k1/4W+PXMi2779u1qbm5WiWYP+Ka5uVmS1N3dXZXzx9icD7tX6/GrGutPpVJ65JFHyv4Vg7n2t4q65wgAmJp27NiRds/ab4QjAHh4e+Fm65Fb6aqp/ra2trSviVu2bJnfJblq/S5gqsr3Owqr6bIIUC6l3H8uv/zytP9X2z5YTfU7PVg7Ozu1YcMGn6tJRzj6pJI3WKDSlXL/qfZ9s5rq37BhQ8WFooPLqgAAWAhHAAAshCMAABbCEQAAC+EIAICFcAQAwEI4AgBgIRwBALAQjgAAWAhHAAAshCMAABbCEQAAC+EIAICl5L/KsXPnzlIvAiirnTt3lvxHWXfu3KmVK1eWdBkAcu/PJQvHq666SpK0Zs2aUi0C8M0VV1xR0nmfOnWKfQcok2z7c8BU049/oSL84Ac/0F133aXf/u3f1ssvv6x58+b5XRLgOnPmjB5++GE988wz+upXv6qvfe1rfpeEKkQ4Ylzef/993XHHHfrVr36laDSqz372s36XBOgXv/iF1q1bpzfeeEPf/OY3tW7dOr9LQpWiQw7G5corr9TBgwe1cOFCff7zn1c0GvW7JExxx44d05IlS3TkyBG98cYbBCMmhHDEuF1yySXau3evmpqadNddd+mpp57yuyRMUW+++aYWLVqk8847T4cOHdJ1113nd0mocoQjJmT69On6xje+oaeeekqPPvqoNmzYoFOnTvldFqaQb37zm1q+fLmWLl2q/fv367d+67f8LgmTAOGIonjooYe0e/du9fb2asWKFfr5z3/ud0mY5JyONw8++KAefvhh7dy5UxdddJHfZWGSoEMOiioWi+nOO+/U+eefr1deeUVXX3213yVhEvrlL3+ppqYm/cu//Iuee+45NTU1+V0SJhnOHFFUn/3sZ3Xo0CHNmjVL9fX1+v73v+93SZhkfvKTn+j666/X22+/re9973sEI0qCcETRzZ07V2+88YaWLVumW265Rd/61rf8LgmTxA9+8APV19erpqZGhw4dUn19vd8lYZIiHFESF1xwgXbs2KGHH35Y999/v/7qr/5KZ8+e9bssVLHnn39ey5cv15IlS/Tmm29q/vz5fpeESYxwRMkEAgE98cQTeuGFF/Tss89q1apVOnHihN9locqcPXtWf/3Xf6377rtPDz30kL7zne/oIx/5iN9lYZKjQw7KwvnKuU9+8pPq6+vjK+eQl1/96le6++679frrr6urq0stLS1+l4QpgnBE2bz//vu68847lUwm9fLLL+tzn/uc3yWhgg0PD6uhoUH/+Z//qd27d2vJkiV+l4QphMuqKJsrr7xSBw4c0DXXXKOlS5dq9+7dfpeECjUwMKDrrrtOZ8+e1VtvvUUwouwIR5TVzJkz9eqrr6qlpUWrVq3iK+eQ4Z/+6Z+0bNkyXXfddTp48KA++clP+l0SpiDCEWVXW1ur9vZ2Pf3003r00Ud1//336//+7//8Lgs+M8boscce0z333KMvf/nL2rNnDx1v4BvuOcJXe/fu1dq1a/UHf/AHeumllzR79my/S4IPTpw4oZaWFr322mvq6OjQfffd53dJmOIIR/junXfe0R133KHzzjtPr776Kl85N8X89Kc/VTAY1PHjx/XSSy/phhtu8LskgMuq8N+1116rQ4cO6dJLL9WiRYu0b98+v0tCmRw6dEh/9Ed/pNOnT+vQoUMEIyoG4YiKMHfuXPX39+vWW2/VihUrtG3bNr9LQon19PToxhtv1Oc+9zkdPHhQV1xxhd8lAS7CERXj/PPP1/bt2/XII49o48aNfOXcJGWMUVtbm5qbm/WlL31JL7/8si6++GK/ywLScM8RFWn79u164IEHdPPNN2v79u30WpwkTpw4ofvuu099fX36h3/4Bz344IN+lwRkRTiiYg0MDGjlypWaN2+eotEov/Be5X76059q5cqVisfj2rVrl5YuXep3SUBOXFZFxVq8eLEGBwd16tQpLVq0SIcPH/a7JIzT22+/rUWLFunDDz/U4OAgwYiKRziiol1xxRU6cOCA6urq9PnPf167du3yuyQUaMeOHfrCF76gz3zmMzp48KCuvPJKv0sCxkQ4ouLNnDlT0WhUDzzwgNasWaMnn3zS75KQB2OM/vZv/1br1q3Txo0b9corr2jmzJl+lwXkpdbvAoB81NTU6O///u+1cOFCfeUrX9HRo0f1jW98QzNmzPC7NGTx4Ycfav369XrppZfU0dGhjRs3+l0SUBA65KDq/PM//7PWrl2ra6+9Vrt379all17qd0nw+I//+A+tXLlS77//vnbt2qWbbrrJ75KAgnFZFVXn1ltv1YEDB3T8+HHV19frxz/+sd8l4dcOHz6sRYsW6Ze//KUOHTpEMKJqEY6oStdcc40GBwc1Z84cLVmyhK+cqwDOxzN+93d/VwMDA7rqqqv8LgkYN8IRVWvOnDnq7+/X7bffrj/5kz9RR0eH3yVNScYYPfHEE1qzZo3Wr1+vV199VbNmzfK7LGBC6JCDqnb++efrhRde0IIFC/SlL31JR48e1VNPPaWamhq/S5sS/vd//1f333+/du7cqX/8x39UKBTyuySgKOiQg0mjt7dX69ev5yvnyuRnP/uZVq5cqaNHj2rHjh1avny53yUBRUM4YlIZHBzUXXfdpcsuu0yvvPKK5s+f73dJk1IsFlMwGNQFF1ygV155RQsWLPC7JKCouOeISaW+vl6HDh2SJC1atEhvvfWWzxVNPrt379YNN9yghQsX6tChQwQjJiXCEZPO/PnzdeDAAf3+7/++brzxRu3cuTNn2w8//LCMlVW+U6dOjTr+ySef1Be/+EW1tLTo9ddf1yWXXFKmyoDyIhwxKV188cWKRqP6sz/7M61du1abN2+WfQfhtdde04UXXqhnn33Wpyorz/XXX69AIKD/+q//Sht+8uRJtbS06Ktf/aqeffZZtbe3q7aW/nyYvLjniEmvo6NDX/7yl7Vu3Tpt27ZN5513nt555x0tXrxYJ06c0Ec+8hEdO3ZMl112md+l+mrv3r267bbbJJ27JL1//37NmDFDIyMjWrlypd599119+9vf1i233OJzpUDpEY6YEr773e9qzZo1uvbaa9XV1aU//uM/1sjIiE6fPq3p06erpaVFzz33nN9l+ubkyZP69Kc/rQ8++EBnzpxRbW2t1q1bp4cffljBYFAzZsxQNBrVpz/9ab9LBcqCcMSU8e677+qOO+5QKpXSL37xi7T7a4FAQG+99Zb+8A//0McK/fPkk0+qra1NZ86ccYcFAgFddtlluuaaa7Rr1y7Nnj3bxwqB8iIcMWUYY7Rq1Sq9/PLLaSEgSbW1taqrq9MPf/hDBQIBnyr0x/DwsBYsWKCTJ09mjJs2bZp27dqlu+66y4fKAP/QIQdTxt/93d9pz549GcEoSadPn9a//uu/6oUXXvChMn899NBDOnv2bNZxxhg1NzfrnXfeKXNVgL84c8SU0N3drZaWloweq16BQECzZ8/WsWPH9NGPfrSM1fnnu9/9rm699dZR29TW1urSSy/Vv/3bv035TkuYOghHTHpnz57N+7tWa2tr9Rd/8ReKRCIlrsp/p06d0u/8zu/o3//937OeTdtuvPFGfe973ytDZYD/uKyKSW/atGnav3+/GhsbNWPGDNXU1GjatOyb/unTp/XMM89Mid+I3LJly6jBWFNTo0AgoIsuukgPPPCAtm3bVuYKAf9w5ogpJZVK6dvf/ra6urr09ttva/r06RnfCjN9+nQtXrxY3//+932qsvSOHz+uq6++Ous3BNXW1urs2bO6+eabdf/997vfoQpMJYQjpqwf/ehHeu655/Stb31LyWRSNTU1On36tDt+x44dWr16tY8Vls6aNWu0Z88e942B8ybhyiuv1IYNG9TS0qJ58+b5XCXgH8IRU96pU6f02muvadu2bdq7d68CgYAbkidOnNCFF17oc4XF9frrr2vFihWSznVCuvDCC9XU1KT169dr8eLFPlcHVAbCEVm99dZbWrRokd9lACXxN3/zN3riiSf8LgMVjG8ORlbvvfeepHOXFqeqeDyuuXPn6rzzzvO7lKL67//+b506dUpz5871uxRfNDc36yc/+YnfZaDCEY4Y1WS954apa8+ePX6XgCrARzkAALAQjgAAWAhHAAAshCMAABbCEQAAC+EIAICFcAQAwEI4AgBgIRwBALAQjgAAWAhHAAAshCMAABbCEQAAC+EIAICFcERJjYyMqLe3Vw0NDX6X4ip3TZW4DgCMjnBEST3++ONqbGxUNBr1uxRXuWsaz/JSqZQCgUAJq/qNQCCQ9TGawcFBtba2KhAIqLW1Vf39/Rk155pvvo/BwcFRl19IvUChCEeUVHt7u98lZCh3TeNZ3v79+0tQSXbGGCUSCfd5MpmUMSZn+8HBQS1evFhLly6VMUbt7e362Mc+ppaWloy2PT09Msa4D+8ynUdPT487LB6Pu22ef/75nDV4xyUSiVHrBcaDcAQqTCqVUldXV1mXOWfOHPf/M2fOHLWtE0zr1q1zh9XV1Wnz5s0Zbb1tclmxYoX7//nz50uSIpGIOjo6NDw8nNF+eHhYV111VdbagWIhHFFUqVRKvb29CgQCamho0NGjR7O2GxkZ0ZYtW9x2/f39OecTCASyhkW2NiMjI0WvaWRkRNFoVA0NDUqlUmptbVVbW1uhqyaDsyynbufSYCQScS/BOq/Nvm8ZjUbdS5pOgDiv0TtMktra2opSr+P48eOSpCNHjqQNr6urS3vuPQsczcyZMzPaLl++XJJ08ODBjPYHDx50xwMlY4Asuru7zXg2j2AwaEKhkEkmk8YYY3p6eoyktHklEgkTDAZNT0+PMcaYffv2GUkmFoulzSccDrvPQ6FQ2nOnTWdnZ9o8g8Ggu+xi1RQMBt32AwMDJhaLmVAoVNB6sZcXiURMPB43xhiTTCZNOBxOG2+399bgrKeBgQEjyYRCITMwMGCMMSYej7vDHOFwOGPd5VNjLrFYzG3b2dmZsb4nugxnfCgUytrWeW351mtramoyTU1NBU+HqYVwRFbjCce+vj4jyQwNDbnDkslkxq4kGUEAAAu7SURBVEHMCScvSe4B3BmfSCTc8QMDAyYYDLrPnfCy20hyA66YNTntCwkCe152+HlrTyQSo4bjRIeNp8bRDA0NueHlrPN81k0h4ej8jZ3gN+ZcMO/bt6/ger0IR+SDcERW4wnHXO/0RzsLsh/e8YUuywk9b4gWq6bxHohzLc+pK1eoVHo4OgYGBtJCsq+vb8LLsN8k2GfBE6nXGMIR+SEckdV4wjHXwSrbWdNo8873ADqRZZWipkLqHRoaSgvkSCQy5vIqMRwdzpn9WAFZaDg6Z/TxeNwkEom0qwKEI0qJDjnwTa6OMcFgUFJmh49sbbJ1wAmFQkWvqdgWLFigvr4+xWIxhUIhbdq0SVu2bCnLssertbVV0rlOQqlUKm1cfX29tm7dKklF/bKDJUuWSDrXCae/v999DpQa4Yii6ezslDR6qHnbvfjii+5B1ukpKv0m+Do6Otzxw8PD7sFZkpqamiRJx44dc4c5bVevXl30morNCZi6ujq1t7crFotp06ZNJVlWMQwODmrp0qXu88OHD2e0cT6G4fz9imH+/PkKh8NqbGzU8ePH3WUAJef3qSsq03guqzo9JYPBoNsT0+lUIc+9I6fzif1wpnF6jnrHhUKhjE41Tu9Up2NLT09PRi/SYtTkHTce3umdWqVznX2cmuLxeNqlVef1JxIJE4lE0ubh3KPMNt9sw/LprTraa3Q6Ojm9ZJ12+/btc2tJJpPuJVBvr+Ox1kOuNt7xTu9Y73zzmVcuXFZFPghHZDXej3LE43G3g0YoFEr7iIT3IBaPx92PL4RCITckHIlEwh0fDofTgtHbprOz0z1I5urcMtGavGHp7eyTLztwnWFO8CnLPUcnEMLhcNbgHm2+9rCxwjHbm4JsD2fdOvMdGhpKW/+5/k6jLWOsNg7vm5585jUawhH5CBjD9y4h0/bt29Xc3MzXcmHSaW5uliR1d3f7XAkqGfccAQCwEI4AAFhq/S4AqFb5/kwSl6aB6kM4AuNE6AGTF5dVAQCwEI4AAFgIRwAALIQjAAAWwhEAAAvhCACAhXAEAMBCOAIAYCEcAQCwEI4AAFgIRwAALIQjAAAWwhEAAAu/yoGsLrzwQkn5/ywTUE3Wr1/vdwmocAHD7+4gi9OnT6uvr09nzpzxu5QpYc2aNfrzP/9z3XDDDX6XMiXU19frE5/4hN9loIIRjkAFCAQC6u7uVlNTk9+lABD3HAEAyEA4AgBgIRwBALAQjgAAWAhHAAAshCMAABbCEQAAC+EIAICFcAQAwEI4AgBgIRwBALAQjgAAWAhHAAAshCMAABbCEQAAC+EIAICFcAQAwEI4AgBgIRwBALAQjgAAWAhHAAAshCMAABbCEQAAC+EIAICFcAQAwEI4AgBgIRwBALAQjgAAWAhHAAAshCMAABbCEQAAC+EIAICl1u8CgKno5z//ecawEydOpA2/6KKLNGPGjHKWBeDXAsYY43cRwFTyyCOP6Otf//qY7WbMmKGTJ0+WoSIANi6rAmX2qU99Kq92V199dYkrAZAL4QiU2apVq1RbO/odjZqaGv3lX/5lmSoCYCMcgTKbPXu2br75ZtXU1ORsM23aNP3pn/5pGasC4EU4Aj64++67let2f21trVasWKFZs2aVuSoADsIR8MGdd96ZsyfqmTNn1NLSUuaKAHgRjoAPLrroIq1cuVLTp0/PGHf++efr9ttv96EqAA7CEfBJc3OzTp06lTZs+vTp+uIXv6gLLrjAp6oASIQj4JtbbrlFH/3oR9OGnTp1Ss3NzT5VBMBBOAI+mTFjhtauXZt2afWSSy7R8uXLfawKgEQ4Ar7yXlqdPn261q1bN+ZnIAGUHl8fB/jo7NmzmjdvnhKJhCTpzTff1A033OBzVQA4cwR8NG3aNPce47x583T99df7XBEAiV/lmPIee+wxvffee36XMaU5v8Rx9uxZrV271udqpraamho9/fTTmjt3rt+lwGdcVp3iAoGAJGn16tU+VzK1vfvuu/r4xz+e0XsV5bVz5051d3erqanJ71LgM84cwcEA+DXnzSLAPUcAACyEIwAAFsIRAAAL4QgAgIVwBADAQjgCAGAhHAEAsBCOAABYCEcAACyEIwAAFsIRAAAL4QgAgIVwBADAQjgCAGAhHDFhIyMj6u3tVUNDg9+luCqxJgDVg99zxIQ9/vjj6ujo8LuMNJVYUyFSqZRmzZqlUv0W+XjmP9pvHUYiES1YsEBf+MIXNHPmzGKUCPiKM0dMWHt7u98lZKjEmgqxf//+ipu/MUaJRMJ9nkwmZYyRMUbLly9XV1eXWlpaNDIyUsxSAV8QjkCFSaVS6urqqsj5z5kzx/2/9wyxrq5O27ZtkyQ9+OCDSqVSEysS8BnhiIKlUin19vYqEAiooaFBR48ezdpuZGREW7Zscdv19/fnnE8gEMh6wM7WJtuZyURrGhkZUTQaVUNDg1KplFpbW9XW1lboqhmzXme49xKlPSwSiSgajaaN89YnSV1dXQoEAmptbU17reOdvyS1tbWN6zU75syZo6985SuKRqMZZ6ajrXfvveFoNOq2GR4eTpuHM72zTu3LvGNtb0BBDKY0Saa7u7ugaYLBoAmFQiaZTBpjjOnp6TGSjHdzSiQSJhgMmp6eHmOMMfv27TOSTCwWS5tPOBx2n4dCobTnTpvOzs60eQaDQXfZxaopGAy67QcGBkwsFjOhUKig9ZJPvYlEIqOueDyeMSzXc6c+Y4xJJpMmFAoZSWZoaGhC8zfGmHA4nLH+s8k2rSOZTBpJaeuukPXurdc7j0gkYuLxuLuMcDhc8PaWj/HsD5icCMcprtCDQV9fX9rB2JjfHBC9BysnnOxlOQdfZ3wikXDHDwwMmGAw6D53DnB2G0nuQbCYNTnt7eDNV771ZguXfMIr27BYLGYkmUgkMuH552usace73seq17tenTcB+S4jX4QjHITjFFfowcA5U8k2H+9w7xmB/fCOL3RZTuh5Q7RYNU0kMAqpt5jhON5pyxmO41nv9jBn3fb09GR98zLWMgp5bYQjjCEcp7xCDwYTOUDnM59iLqsUNZWq3skSjs6bAe8Z23jWuz1saGgoLQC9Z8r5LCNfhCMcdMhBSeXqGBMMBiVJR44cyTmt0yZbB5xQKFT0miaqVPXmo9Tzz9fhw4clSTfddFPGuIms9wULFqivr0+xWEyhUEibNm3Sli1biroMwItwREE6OzsljR5q3nYvvvii263f6U0o/SZIOjo63PHDw8NqbW1159HU1CRJOnbsmDvMabt69eqi1zRR+dZbTE4Y3HbbbSWZfyFGRkb0zDPPKBgMatmyZe7wYqz3QCCgVCqluro6tbe3KxaLadOmTUVdBpDG71NX+EsFXkZyehIGg0G396DTEUX6TQ9Db69J78OZxuld6B0XCoUyOtU4vT2dzhg9PT0ZvUiLUVO2Xp6Fyrdeu4ep02nHW6uzbhKJhHsJ0WnjdO5xem1672dOZP759Fb1dnTy3vtzep56X7sj3/XuzM+7DGde+vWlWufvG4/H0y6tjrW95avQ/QGTF+E4xY3nYBCPx90DcCgUSutG7z0wxuNxt8t9KBTKOFAlEgl3fDgcTgtGb5vOzs60YMjWIWOiNXkPqHbYFCKfeuPxuBtOfX19xhiTUavTCzUcDqcFhJT+EYjOzs6izX+scMwWPs4jEom4H8XIJp/17rwxyTXMCXJnefkuoxCEIxwBY0r05Y2oCoFAQN3d3e4lQVQu50Pv7LKlw/4AB/ccAQCwEI5AFfD2gOWLvYHS4yergFGM9jNNXqW+1Hn55Zen/Z9Lq0BpEY7AKColhCqlDmCq4LIqAAAWwhEAAAvhCACAhXAEAMBCOAIAYCEcAQCwEI4AAFgIRwAALIQjAAAWwhEAAAvhCACAhXAEAMBCOAIAYOFXOaDm5mbt2bPH7zIAoGIEDL+FM6U99thjeu+99/wuA6gINTU1evrppzV37ly/S4HPCEcAACzccwQAwEI4AgBgIRwBALAQjgAAWP4fUlerrIUOTMgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "import pydot\n",
    "# 產生網絡拓撲圖\n",
    "plot_model(model, to_file='seq2seq_graph.png')\n",
    "Image('seq2seq_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5.訓練模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 1.9883 - val_loss: 2.4889\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 1.8558 - val_loss: 2.3938\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 1.7530 - val_loss: 2.2534\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 1.6580 - val_loss: 2.1772\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 1.5677 - val_loss: 2.0979\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 1.4879 - val_loss: 1.9950\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 1.4149 - val_loss: 1.9871\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 28s 3ms/step - loss: 1.3529 - val_loss: 1.8938\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 1.2989 - val_loss: 1.8619\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 1.2525 - val_loss: 1.8363\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 1.2117 - val_loss: 1.8060\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 1.1746 - val_loss: 1.7819\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 1.1389 - val_loss: 1.7680\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 1.1060 - val_loss: 1.7480\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 1.0731 - val_loss: 1.7451\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 1.0473 - val_loss: 1.7262\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 28s 3ms/step - loss: 1.0213 - val_loss: 1.7279\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 28s 3ms/step - loss: 0.9951 - val_loss: 1.7036\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.9645 - val_loss: 1.7078\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.9405 - val_loss: 1.6996\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.9142 - val_loss: 1.7065\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.8908 - val_loss: 1.6983\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.8688 - val_loss: 1.7027\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.8473 - val_loss: 1.6932\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 28s 3ms/step - loss: 0.8256 - val_loss: 1.6998\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.8078 - val_loss: 1.6911\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.7862 - val_loss: 1.6983\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.7653 - val_loss: 1.6958\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.7491 - val_loss: 1.7082\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.7296 - val_loss: 1.7029\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.7114 - val_loss: 1.7086\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.6959 - val_loss: 1.7225\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.6779 - val_loss: 1.7257\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.6610 - val_loss: 1.7234\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.6460 - val_loss: 1.7367\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.6290 - val_loss: 1.7405\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.6142 - val_loss: 1.7440\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.6034 - val_loss: 1.7487\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.5863 - val_loss: 1.7624\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.5721 - val_loss: 1.7576\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.5581 - val_loss: 1.7717\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.5472 - val_loss: 1.7703\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.5341 - val_loss: 1.7747\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.5230 - val_loss: 1.7889\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.5101 - val_loss: 1.8002\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.4974 - val_loss: 1.8033\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.4871 - val_loss: 1.8112\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.4757 - val_loss: 1.8074\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.4661 - val_loss: 1.8234\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 28s 3ms/step - loss: 0.4553 - val_loss: 1.8271\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.4467 - val_loss: 1.8335\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.4354 - val_loss: 1.8385\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.4259 - val_loss: 1.8454\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.4168 - val_loss: 1.8586\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.4083 - val_loss: 1.8683\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.3993 - val_loss: 1.8695\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.3916 - val_loss: 1.8780\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.3831 - val_loss: 1.8885\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.3751 - val_loss: 1.8917\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 28s 3ms/step - loss: 0.3684 - val_loss: 1.8990\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.3611 - val_loss: 1.9016\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.3558 - val_loss: 1.9079\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.3469 - val_loss: 1.9139\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.3395 - val_loss: 1.9315\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.3336 - val_loss: 1.9435\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.3268 - val_loss: 1.9326\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.3200 - val_loss: 1.9374\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.3143 - val_loss: 1.9377\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 28s 3ms/step - loss: 0.3082 - val_loss: 1.9547\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.3018 - val_loss: 1.9694\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.2962 - val_loss: 1.9698\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.2908 - val_loss: 1.9768\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.2853 - val_loss: 1.9772\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.2797 - val_loss: 1.9937\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.2758 - val_loss: 1.9891\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 0.2700 - val_loss: 2.0087\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.2649 - val_loss: 2.0160\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.2582 - val_loss: 2.0135\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.2543 - val_loss: 2.0175\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.2498 - val_loss: 2.0216\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.2454 - val_loss: 2.0254\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.2402 - val_loss: 2.0406\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.2360 - val_loss: 2.0516\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 0.2322 - val_loss: 2.0522\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 28s 3ms/step - loss: 0.2273 - val_loss: 2.0550\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.2232 - val_loss: 2.0655\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.2187 - val_loss: 2.0719\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 28s 3ms/step - loss: 0.2138 - val_loss: 2.0797\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.2104 - val_loss: 2.0752\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.2059 - val_loss: 2.0785\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.2024 - val_loss: 2.0891\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.1983 - val_loss: 2.1021\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.1944 - val_loss: 2.1005\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.1900 - val_loss: 2.0983\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.1867 - val_loss: 2.1084\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.1832 - val_loss: 2.1234\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.1799 - val_loss: 2.1207\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 0.1761 - val_loss: 2.1267\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 0.1717 - val_loss: 2.1308\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 0.1683 - val_loss: 2.1358\n"
     ]
    }
   ],
   "source": [
    "# 設定模型超參數\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# 開始訓練\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# 儲存模型\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6.模型預測\n",
    "以下是預測階段的步驟:\n",
    "\n",
    "對輸入進行編碼(encode)並取得解碼器所需要的初始狀態(initial decoder state)\n",
    "\n",
    "以此初始狀態運行一步解碼器，並以“開始序列”標記作為目標。輸出將是下一個目標標記\n",
    "\n",
    "重複當前目標標記和當前狀態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 嗨。\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 嗨。\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: 你用跑的。\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: 等等！\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: 你好。\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: 讓我來。\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: 我想要果汁。\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: 不會吧。\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: 乾杯!\n",
      "\n",
      "-\n",
      "Input sentence: He ran.\n",
      "Decoded sentence: 他跑了。\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: 跳進來。\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: 我會做的。\n",
      "\n",
      "-\n",
      "Input sentence: I quit.\n",
      "Decoded sentence: 我買了一輛小後。\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: 我很好奇。\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: 聽著。\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: 不可能！\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: 不可能！\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: 你確定？\n",
      "\n",
      "-\n",
      "Input sentence: Try it.\n",
      "Decoded sentence: 試試吧。\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: 我們來試試。\n",
      "\n",
      "-\n",
      "Input sentence: Why me?\n",
      "Decoded sentence: 為什麼是我？\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: 去問湯姆。\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: 友善點。\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: 公平點。\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: 友善點。\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: 對湯姆點。\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: 聯繫我。\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: 聯繫我們。\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: 進來。\n",
      "\n",
      "-\n",
      "Input sentence: Get Tom.\n",
      "Decoded sentence: 找到湯姆。\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: 滾出去！\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Go away.\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: 再見！\n",
      "\n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: 再見！\n",
      "\n",
      "-\n",
      "Input sentence: Hang on!\n",
      "Decoded sentence: 等一下！\n",
      "\n",
      "-\n",
      "Input sentence: He came.\n",
      "Decoded sentence: 他來了。\n",
      "\n",
      "-\n",
      "Input sentence: He runs.\n",
      "Decoded sentence: 他跑。\n",
      "\n",
      "-\n",
      "Input sentence: Help me.\n",
      "Decoded sentence: 幫我一下。\n",
      "\n",
      "-\n",
      "Input sentence: Hold on.\n",
      "Decoded sentence: 堅持。\n",
      "\n",
      "-\n",
      "Input sentence: Hug Tom.\n",
      "Decoded sentence: 抱抱湯姆！\n",
      "\n",
      "-\n",
      "Input sentence: I agree.\n",
      "Decoded sentence: 我同意。\n",
      "\n",
      "-\n",
      "Input sentence: I'm ill.\n",
      "Decoded sentence: 我很好。\n",
      "\n",
      "-\n",
      "Input sentence: I'm old.\n",
      "Decoded sentence: 我是個不成的工作。\n",
      "\n",
      "-\n",
      "Input sentence: It's OK.\n",
      "Decoded sentence: 這是不可能的。\n",
      "\n",
      "-\n",
      "Input sentence: It's me.\n",
      "Decoded sentence: 它是我們的錯誤。\n",
      "\n",
      "-\n",
      "Input sentence: Join us.\n",
      "Decoded sentence: 來加入我們吧。\n",
      "\n",
      "-\n",
      "Input sentence: Keep it.\n",
      "Decoded sentence: 留著吧。\n",
      "\n",
      "-\n",
      "Input sentence: Kiss me.\n",
      "Decoded sentence: 吻我。\n",
      "\n",
      "-\n",
      "Input sentence: Perfect!\n",
      "Decoded sentence: 完美！\n",
      "\n",
      "-\n",
      "Input sentence: See you.\n",
      "Decoded sentence: 再見！\n",
      "\n",
      "-\n",
      "Input sentence: Shut up!\n",
      "Decoded sentence: 閉嘴！\n",
      "\n",
      "-\n",
      "Input sentence: Skip it.\n",
      "Decoded sentence: 不管它。\n",
      "\n",
      "-\n",
      "Input sentence: Take it.\n",
      "Decoded sentence: 當然!\n",
      "\n",
      "-\n",
      "Input sentence: Wake up!\n",
      "Decoded sentence: 醒醒！\n",
      "\n",
      "-\n",
      "Input sentence: Wash up.\n",
      "Decoded sentence: 在這等著。\n",
      "\n",
      "-\n",
      "Input sentence: We know.\n",
      "Decoded sentence: 我們知道。\n",
      "\n",
      "-\n",
      "Input sentence: Welcome.\n",
      "Decoded sentence: 歡迎。\n",
      "\n",
      "-\n",
      "Input sentence: Who won?\n",
      "Decoded sentence: 誰贏了？\n",
      "\n",
      "-\n",
      "Input sentence: Why not?\n",
      "Decoded sentence: 為什麼不？\n",
      "\n",
      "-\n",
      "Input sentence: You run.\n",
      "Decoded sentence: 你跑。\n",
      "\n",
      "-\n",
      "Input sentence: Back off.\n",
      "Decoded sentence: 往後退點。\n",
      "\n",
      "-\n",
      "Input sentence: Be still.\n",
      "Decoded sentence: 試試吧。\n",
      "\n",
      "-\n",
      "Input sentence: Cuff him.\n",
      "Decoded sentence: 把他銬上。\n",
      "\n",
      "-\n",
      "Input sentence: Drive on.\n",
      "Decoded sentence: 開車。\n",
      "\n",
      "-\n",
      "Input sentence: Get away!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get away!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get down!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get lost!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get real.\n",
      "Decoded sentence: 醒醒吧。\n",
      "\n",
      "-\n",
      "Input sentence: Grab Tom.\n",
      "Decoded sentence: 抓住湯姆。\n",
      "\n",
      "-\n",
      "Input sentence: Grab him.\n",
      "Decoded sentence: 抓住他。\n",
      "\n",
      "-\n",
      "Input sentence: Have fun.\n",
      "Decoded sentence: 玩得開心。\n",
      "\n",
      "-\n",
      "Input sentence: He tries.\n",
      "Decoded sentence: 他來試試。\n",
      "\n",
      "-\n",
      "Input sentence: Humor me.\n",
      "Decoded sentence: 你就隨了我的意吧。\n",
      "\n",
      "-\n",
      "Input sentence: Hurry up.\n",
      "Decoded sentence: 快點！\n",
      "\n",
      "-\n",
      "Input sentence: Hurry up.\n",
      "Decoded sentence: 快點！\n",
      "\n",
      "-\n",
      "Input sentence: I forgot.\n",
      "Decoded sentence: 我忘了。\n",
      "\n",
      "-\n",
      "Input sentence: I resign.\n",
      "Decoded sentence: 我記得這個了。\n",
      "\n",
      "-\n",
      "Input sentence: I'll pay.\n",
      "Decoded sentence: 我會告訴我的妻子。\n",
      "\n",
      "-\n",
      "Input sentence: I'm busy.\n",
      "Decoded sentence: 我很好奇。\n",
      "\n",
      "-\n",
      "Input sentence: I'm cold.\n",
      "Decoded sentence: 我是義大利人。\n",
      "\n",
      "-\n",
      "Input sentence: I'm fine.\n",
      "Decoded sentence: 我很好。\n",
      "\n",
      "-\n",
      "Input sentence: I'm full.\n",
      "Decoded sentence: 我很好。\n",
      "\n",
      "-\n",
      "Input sentence: I'm sick.\n",
      "Decoded sentence: 我生病了。\n",
      "\n",
      "-\n",
      "Input sentence: I'm sick.\n",
      "Decoded sentence: 我生病了。\n",
      "\n",
      "-\n",
      "Input sentence: Leave me.\n",
      "Decoded sentence: 讓我一個人呆會兒。\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 我們現在去吧。\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 我們現在去吧。\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 我們現在去吧。\n",
      "\n",
      "-\n",
      "Input sentence: Look out!\n",
      "Decoded sentence: 當心！\n",
      "\n",
      "-\n",
      "Input sentence: She runs.\n",
      "Decoded sentence: 她跑。\n",
      "\n",
      "-\n",
      "Input sentence: Stand up.\n",
      "Decoded sentence: 起立。\n",
      "\n",
      "-\n",
      "Input sentence: They won.\n",
      "Decoded sentence: 他們贏了。\n",
      "\n",
      "-\n",
      "Input sentence: Tom died.\n",
      "Decoded sentence: 湯姆不會游泳。\n",
      "\n",
      "-\n",
      "Input sentence: Tom quit.\n",
      "Decoded sentence: 湯姆不會快。\n",
      "\n",
      "-\n",
      "Input sentence: Tom swam.\n",
      "Decoded sentence: 湯姆笑了。\n",
      "\n",
      "-\n",
      "Input sentence: Trust me.\n",
      "Decoded sentence: 相信我。\n",
      "\n",
      "-\n",
      "Input sentence: Try hard.\n",
      "Decoded sentence: 努力。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定義要進行取樣的模型\n",
    "\n",
    "# 定義編碼器(encoder)的模型\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 定義解碼器LSTM cell的初始權重輸入\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# # 解碼器(decoder)定義初始狀態(initial decoder state)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs) #我們使用`decoder_states_inputs`來做為初始值(initial state)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 定義解碼器(decoder)的模型\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "# 反向查找字符索引來將序列解碼為可讀的內容。\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "# 對序列進行解碼\n",
    "def decode_sequence(input_seq):\n",
    "    # 將輸入編碼成為state向量\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # 產生長度為1的空白目標序列\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    \n",
    "    # 發佈特定的目標序列起始字符\"[SOS]\",在這個範例中是使用 \"\\t\"字符\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # 對批次的序列進行抽樣迴圈\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # 對符標抽樣\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # 停止迴圈的條件: 到達最大的長度或是找到\"停止[EOS]\"字符,在這個範例中是使用 \"\\n\"字符\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 更新目標序列(of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 更新 states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # 從訓練集中取出一個序列並試著解碼\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
